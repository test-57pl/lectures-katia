# проект (12.12-19.12)

### Группы:

1. **Соня, Ира**: *Властелин Колец*
2. **Глаша, Ксюша, Лена**:
3. **Даша, Клементина**:
4. **Артем, Илья**

## Предзадание:

Создайте команду из 2-х или 3-х человек. Выберите одно большое литературное произведение. Скачайте его в формате `.txt`. Если произведение состоит из нескольких частей-файлов, то соберите их в один: автоматически (лучше) или вручную (хуже, но тоже сойдет). 

Загрузите ваш файл на *GitHub* и пользуйтесь ссылкой на скачивание для загрузки (`wget.download()`).

Можете работать в одном *Google Colab*’е на всех или каждый в своем, но помните, что тетрадки я тоже буду смотреть, так что лучше работайте всю неделю в одном месте, чтобы ничего потом не потерять. Я скорее не буду снижать оценки за качество кода, но хороший код может дать вам бонус в карму (и в оценку). Перед каждым кусочком кода пишите, какое задание вы сейчас выполняете и печатайте ответы на него.

## Задание:

1. `with open(…) as …:` `'r'` `.read()`<br>
`nltk`: `.sent_tokenize()`<br>
`len()`<br>
При помощи `nltk` cоздайте отдельный список предложений в переменной (`text_sentences`). Сколько предложений в вашем тексте?
2. `nltk`: `.word_tokenize()`<br>
При помощи `nltk` cоздайте отдельный список токенов (~слов) в переменной (`text_tokens`).
3. `nltk`: `stopwords.words()`<br>
`.lower()` `string.punctuation` `.isnumeric()`<br>
`with open(…) as …:` `'w'` `.write() / .writelines()` `'\t'.join(list)`<br>
Создайте новый список токенов в нижнем регистре **без** стоп-слов, пунктуации и чисел и запишите его в переменную (`text_tokens_clear`). Сохраните этот список в файл `text_tokens_clear.txt`, где каждый токен будет отделен от следующего знаком табуляции (`'\t'`).
**NB!** Cледите, пожалуйста, внимательно за пунктуацией, чтобы удалить **все** лишнее! А еще помните при поиске, что стоп-слова в `nltk` все в нижнем регистре. 
Сохранить в файл **очень** важно, потому что, если у вас что-то сломается, то всегда можно будет откатиться до старой версии.<br><br>Дальше мы будем работать уже со списком `text_tokens_clear`.
1. `len()`<br>
Сколько токенов в `text_tokens_clear`? Сколько процентов от количества изначальных токенов в `text_tokens` составляет количество “почищенных” токенов? 
2. `set()` `len()`<br>
Сколько уникальных токенов в вашем тексте? Запишите их в список `text_unique_tokens`.
3. `dict[key]` `key=itemgetter(1)` `sorted()` `reverse=True`<br>
`dict.keys()` `list()` `list[:21]`<br>
Создайте частотный словарь токенов в `text_tokens_clear`. В этом вам поможет создание словаря `dict_tokens`, где каждому токену будет соответствовать число — количество таких токенов в `text_tokens_clear`. Отсортируйте его так, чтобы сверху находились те токены, которых больше всего.Выведите топ-20 самых частотных токенов в вашем тексте? Что это говорит о выбранном вами тексте?
4. `pymorphy2`:  `morph.parse(…)[0]` `.tag.POS` `.normal_form`<br>
`with open(…) as …:` `'w'` `.write() / .writelines()` `'\t'.join(list)`<br>
Разберите каждое слово из списка `text_unique_tokens` при помощи `pymorphy2`(всегда берите самый частотный по `score`, то есть первый разбор из списка). Запишите его в отдельный файл `tokens_pos.txt`, где каждая строка должна состоять из токена, его начальной формы и частеречной принадлежности, разделенных пробелами, например:
    
    ```
    ...
    скачу скакать VERB
    коню конь NOUN
    ...
    ```
    
5. `pymorphy2`:  `morph.parse(…)[0]` `.normal_form`<br>
`set()` `len()`<br>
Создайте список `text_lemmas`, в котором будут находиться начальные формы (~леммы) всех слов из списка `text_tokens_clear`. Сколько уникальных лемм в тексте?
**NB!** Слова в списке начальных форм **могут** повторяться!
6. `pymorphy2`:  `morph.parse(…)[0]` `.tag.POS`<br>
`len()`<br>
Разберите каждое слово из списка `text_lemmas` при помощи `pymorphy2`(всегда берите самый частотный по `score`, то есть первый разбор из списка). Составьте словарь, где каждой части речи будет соответствовать число слов этой части речи. Какую долю от всех слов составляет каждая часть речи? Например, для глагола — это количество глаголов, деленное на общее число слов в тексте. Что это говорит о вашей книге?
7. `pymorphy2`:  `morph.parse(…)[0]` `.tag.POS` `{...} in form.tag`<br>
`dict[key]` `key=itemgetter(1)` `sorted()` `reverse=True`<br>
`dict.keys()` `list()` `list[:21]`<br>
Найдите топ-20 (по частотности) глаголов, существительных и наречий в списке `text_lemmas`. Составьте три словаря: `dict_verbs`, `dict_nouns` и `dict_adverbs`, — в каждом из которых будут только слова соответствующей части речи. Ключом будет начальная форма из `text_lemmas`, а значением — число этого слова в `text_lemmas`. Отсортируйте полученный словарь по значению (в порядке убывания). Что это говорит о вашем тексте?
8. `pymorphy`: `.word_is_known()`<br>
`len()`<br>
Создайте список `unknown_words`, в котором бы находились все токены из `text_unique_tokens`, которых не знает `pymorphy2`**.** Сколько таких токенов? Какие это слова и почему, по-вашему, их не знает `pymorphy2`?
9. `pymorphy`: `.lexeme` `.tag.case` `morph.lat2cyr()` `{...} in form.tag`<br>
`'\t'.join(list)` `print(sep='\t')`<br>
Выберите из списка `unknown_words` одно существительное и выведите его склонение на экран, где каждая строчка будет состоять из кириллического названия падежа в `pymorphy2`, слова в этом падеже в единственном числе и слова в этом падеже во множественном числе, разделенных табуляцией (`'\t'`), например:
    
    ```
    им  гендальф    гендальфы
    рд  гендальфа   гендальфов
    дт  гендальфу   гендальфам
    вн  гендальфа   гендальфах
    тв  гендальфом  гендальфами
    пр  гендальфе   гендальфах
    ```
    
10. (**Со звездочкой**)
Когда мы хотим что-то понять про текст, нам интересно посмотреть не только на слова, но и на их сочетания. Для таких сочетаний есть название *n*-граммы **(читается как “эн-граммы”) — это последовательности из *n* слов, которые в изначальном тексте идут подряд. Посмотрите [документацию](https://tedboy.github.io/nlps/generated/nltk.html) для  `nltk` и найдите в нем функции для создания биграмм (*bigrams*, *n* = 2) и триграмм (*trigrams*, *n* = 3). 
Важно, что в этот момент нам становятся нужны предлоги и всякие другие стоп-слова (и это логично, ведь например, *шагать по краю* и *************шагать к краю************* — это разные по значению сочетания, но без стоп-слов *по* и *к* они бы выглядели одинаково: ***********шагать краю***********). Поэтому нам надо:
    1.  вернуться к изначальному списку токенов `text_tokens`;
    2. вновь почистить его от пунктуации и чисел, **но** не от стоп-слов;
    3. каждое слово в списке заменить на его начальную форму (так как нам хочется, чтобы сочетания типа *люблю покушать* и *любила покушать* считались чем-то одинаковым: *любить покушать*);
    4. полученный список токенов прогнать через найденные функции!
    
    Составьте топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (т.е. приведенном к начальной форме). Как вы думаете, почему получаются именно такие *n*-граммы?
    

## Презентация:

Это должна быть **презентация** (желательно *Google Slides*, чтобы у всех был доступ). Рассказывать могут все / кто-то один: разницы особо нет!

**Что должно быть в презентации?**

1. Распределение “ролей”: кто что делал из заданий?
2. Какой текст вы выбрали?
3. Какая-то общая статистика: типа цифр из пунктов (1), (4), (5), (8).<br><br>
Хороший тон: строить диаграммы (например, в *Google Slides* их можно прямо через “Вставку” добавить в презентацию). Это не обязательно, но будет очень круто!
Основные типы диаграмм — это: **круговые** (когда у нас суммарно данные складываются в 100%, например как в (4) или (9)) и **столбчатые** (когда хочется показать, сколько одного по сравнению с другим, например, если выводить не только слова, но и их количества в (6) или (10)). <br><br>
1. Для заданий, где есть топ-*х* ((6), (10) или (13)) расскажите про те слова, которые кажутся вам интересными в контексте вашей книги. (например, “*У нас в топ-20 существительных попало слово ‘хоббит’! Нам кажется, что это интересно, потому что слово должно быть очень редким в других текстах, но у нас текст про хоббитов, поэтому его там много!*” или “*В нашем топе аж 3 устаревших слова, потому что текст написан в 19-м веке*”). То же касается выводов из пунктов (9) и (11) (например, “*В книге больше всего глаголов, потому что это боевик. Джеймс Бонд постоянно что-то делает: то стреляет, то спасает девушек, то гоняет на тачках*” или “*У нас все неизвестные слова — это имена героев, потому что книжка иностранная, а имена выдуманные!*”)
2. Расскажите про проблемы, с которыми столкнулись при работе. Например, что в (12)-м задании все неправильно склонялось. Или что в топе какие-то несуществующие слова. Или что пунктуацию не удалось автоматически убрать.


## Дедлайн:

К уроку в пн (19.12 10:00) в форме ([здесь](https://docs.google.com/forms/d/e/1FAIpQLSfsRKrQzPN0WTtS55S5D4OlbzVIy15XdVmvOUsXbxLIJmRWvw/viewform?usp=sf_link) ссылка) должны быть ссылки на все ваши тетрадки и презентацию.
